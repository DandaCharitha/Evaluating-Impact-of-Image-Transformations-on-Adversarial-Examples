{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f933610-438e-4323-9b52-b803dedd943a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DandaC8719\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\DandaC8719\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Step 1: Load the pre-trained ResNet50 model\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Load the images\n",
    "input_directory = \"./output_images_folder_50000\"\n",
    "output_original_directory = \"./original_incorrect_predictions_folder(FGSM)\"\n",
    "output_adversarial_directory = \"./adversarial_images_folder(FGSM)\"\n",
    "\n",
    "input_images = [os.path.join(input_directory, filename) for filename in os.listdir(input_directory) if filename.endswith(\".JPEG\")]\n",
    "\n",
    "# Step 3: Define the FGSM attack function\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    perturbed_image = image + epsilon * sign_data_grad\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image\n",
    "\n",
    "# Step 4: Generate adversarial examples\n",
    "epsilon = 0.01  # Adjust this value as needed\n",
    "successful_adversarial_examples = []\n",
    "corres_org_img=[]\n",
    "\n",
    "for input_image in input_images:\n",
    "    # Load the original image\n",
    "    original_image = Image.open(input_image).convert(\"RGB\")\n",
    "    \n",
    "    # Convert the PIL image to a PyTorch tensor and apply the necessary preprocessing\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    image_tensor = preprocess(original_image).unsqueeze(0)\n",
    "    image_tensor.requires_grad = True\n",
    "\n",
    "    # Forward pass to get the predictions\n",
    "    output = model(image_tensor)\n",
    "    original_label = torch.argmax(output, 1).item()\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = nn.CrossEntropyLoss()(output, torch.tensor([original_label]))\n",
    "\n",
    "    # Zero all existing gradients\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backpropagate the loss to compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Collect the gradients of the input image\n",
    "    data_grad = image_tensor.grad.data\n",
    "\n",
    "    # Generate the perturbed image using the FGSM attack\n",
    "    perturbed_image = fgsm_attack(image_tensor, epsilon, data_grad)\n",
    "\n",
    "    # Forward pass with the perturbed image\n",
    "    perturbed_output = model(perturbed_image)\n",
    "\n",
    "    # Check if the perturbed image's predicted label differs from the original label\n",
    "    \n",
    "    perturbed_label = torch.argmax(perturbed_output, 1).item()\n",
    "    if perturbed_label != original_label:\n",
    "        # Save the perturbed image as a successful adversarial example\n",
    "        successful_adversarial_examples.append(perturbed_image)\n",
    "        corres_org_img.append(input_image)\n",
    "        # Save the original image to output_original_directory\n",
    "        os.makedirs(output_original_directory, exist_ok=True)\n",
    "        original_image_name = os.path.basename(input_image)\n",
    "        output_original_image_path = os.path.join(output_original_directory, original_image_name)\n",
    "        original_image.save(output_original_image_path)\n",
    "        \n",
    "        # Save the perturbed image to output_adversarial_directory\n",
    "        os.makedirs(output_adversarial_directory, exist_ok=True)\n",
    "        perturbed_image_name = f\"perturbed_{original_image_name}\"\n",
    "        output_perturbed_image_path = os.path.join(output_adversarial_directory, perturbed_image_name)\n",
    "        perturbed_image_pil = transforms.ToPILImage()(perturbed_image.squeeze(0))\n",
    "        perturbed_image_pil.save(output_perturbed_image_path)\n",
    "\n",
    "# Print the length of successful adversarial examples\n",
    "print(\"Number of successful adversarial examples:\", len(successful_adversarial_examples))\n",
    "print(len(corres_org_img))\n",
    "\n",
    "# # Show the original and adversarial images for all successful examples\n",
    "# for idx, perturbed_image in enumerate(successful_adversarial_examples):\n",
    "#     # Load the original image\n",
    "#     original_image_path = corres_org_img[idx]\n",
    "#     try:\n",
    "#         original_image = Image.open(original_image_path).convert(\"RGB\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading original image {original_image_path}: {e}\")\n",
    "#         continue\n",
    "\n",
    "#     # Convert the perturbed image to a numpy array\n",
    "#     perturbed_image_np = perturbed_image.squeeze().detach().cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "#     # Get the original label\n",
    "#     with torch.no_grad():\n",
    "#         original_image_tensor = preprocess(original_image).unsqueeze(0)\n",
    "#         original_output = model(original_image_tensor)\n",
    "#         original_label_idx = torch.argmax(original_output, 1).item()\n",
    "\n",
    "#     # Get the adversarial label\n",
    "#     with torch.no_grad():\n",
    "#         perturbed_output = model(perturbed_image)\n",
    "#         perturbed_label_idx = torch.argmax(perturbed_output, 1).item()\n",
    "\n",
    "#     fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "#     # Plot the original image\n",
    "#     axes[0].imshow(original_image)\n",
    "#     axes[0].set_title(f\"Original Image {idx+1}, Label: {original_label_idx}\")\n",
    "#     axes[0].axis('off')\n",
    "\n",
    "#     # Plot the adversarial image\n",
    "#     axes[1].imshow(perturbed_image_np)\n",
    "#     axes[1].set_title(f\"Adversarial Image {idx+1}, Label: {perturbed_label_idx}\")\n",
    "#     axes[1].axis('off')\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6871cb6e-a800-4125-930f-1711e1c38448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Label ID: 985\n",
      "Original Label Name: ['n11939491', 'daisy']\n",
      "Original Confidence: 97.21648097038269\n",
      "Adversarial Label ID: 301\n",
      "Adversarial Label Name: ['n02165456', 'ladybug']\n",
      "Adversarial Confidence: 13.377849757671356\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# Step 1: Load the pre-trained ResNet50 model\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Load ImageNet class labels\n",
    "with open(\"C:/Users/DandaC8719/Documents/capstone_overall_code/imagenet_class_index.json\", \"r\") as f:\n",
    "    imagenet_classes = json.load(f)\n",
    "\n",
    "# Convert class labels to dictionary format\n",
    "imagenet_classes = {int(k): v for k, v in imagenet_classes.items()}\n",
    "\n",
    "# Step 3: Define the FGSM attack function\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    perturbed_image = image + epsilon * sign_data_grad\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image\n",
    "    \n",
    "# Step 4: Generate adversarial examples\n",
    "epsilon = 0.01  # Adjust this value as needed\n",
    "\n",
    "def get_labels(image_path):\n",
    "    from PIL import Image  # Import Image module here\n",
    "    \n",
    "    # Load the original image\n",
    "    original_image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Convert the PIL image to a PyTorch tensor and apply the necessary preprocessing\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    image_tensor = preprocess(original_image).unsqueeze(0)\n",
    "    image_tensor.requires_grad = True\n",
    "\n",
    "    # Forward pass to get the predictions\n",
    "    output = model(image_tensor)\n",
    "    original_label_id = torch.argmax(output, 1).item()\n",
    "\n",
    "    # Calculate the softmax probabilities for the predictions\n",
    "    probabilities = nn.functional.softmax(output, dim=1)[0]\n",
    "\n",
    "    # Get class names for original label\n",
    "    original_label_name = imagenet_classes[original_label_id]\n",
    "\n",
    "    # Get the confidence percentage for the original prediction\n",
    "    original_confidence = probabilities[original_label_id].item() * 100\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = nn.CrossEntropyLoss()(output, torch.tensor([original_label_id]))\n",
    "\n",
    "    # Zero all existing gradients\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backpropagate the loss to compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Collect the gradients of the input image\n",
    "    data_grad = image_tensor.grad.data\n",
    "\n",
    "    # Generate the perturbed image using the FGSM attack\n",
    "    perturbed_image = fgsm_attack(image_tensor, epsilon, data_grad)\n",
    "\n",
    "    # Forward pass with the perturbed image\n",
    "    perturbed_output = model(perturbed_image)\n",
    "\n",
    "    # Get the adversarial label\n",
    "    perturbed_label_id = torch.argmax(perturbed_output, 1).item()\n",
    "\n",
    "    # Get class names for adversarial label\n",
    "    perturbed_label_name = imagenet_classes[perturbed_label_id]\n",
    "    \n",
    "    # Calculate the softmax probabilities for the adversarial predictions\n",
    "    perturbed_probabilities = nn.functional.softmax(perturbed_output, dim=1)[0]\n",
    "\n",
    "    # Get the confidence percentage for the adversarial prediction\n",
    "    perturbed_confidence = perturbed_probabilities[perturbed_label_id].item() * 100\n",
    "    \n",
    "    return (original_label_id, original_label_name, original_confidence), (perturbed_label_id, perturbed_label_name, perturbed_confidence)\n",
    "\n",
    "# Example usage:\n",
    "image_path = \"./output_images_folder_50000/ILSVRC2012_val_00040021.JPEG\"  \n",
    "(original_label_id, original_label_name, original_confidence), (adversarial_label_id, adversarial_label_name, adversarial_confidence) = get_labels(image_path)\n",
    "print(\"Original Label ID:\", original_label_id)\n",
    "print(\"Original Label Name:\", original_label_name)\n",
    "print(\"Original Confidence:\", original_confidence)\n",
    "print(\"Adversarial Label ID:\", adversarial_label_id)\n",
    "print(\"Adversarial Label Name:\", adversarial_label_name)\n",
    "print(\"Adversarial Confidence:\", adversarial_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f613311-e5a9-404a-a2f3-5b3fbfd04716",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
