{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cf235dd-0a99-47c5-bbeb-11b2c60d024c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DandaC8719\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\DandaC8719\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load the pre-trained DenseNet121 model\n",
    "model = models.densenet121(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Load the images\n",
    "input_directory = \"./output_images_folder(DenseNet)\"\n",
    "output_original_directory = \"./original_incorrect_predictions_folder(PGD)_DenseNet\"\n",
    "output_adversarial_directory = \"./adversarial_images_folder(PGD)_DenseNet\"\n",
    "\n",
    "input_images = [os.path.join(input_directory, filename) for filename in os.listdir(input_directory) if filename.endswith(\".JPEG\")]\n",
    "\n",
    "# Step 3: Define the PGD attack function\n",
    "def pgd_attack(model, image, target, epsilon, alpha, num_iter):\n",
    "    perturbed_image = image.clone().detach().requires_grad_(True)\n",
    "    for i in range(num_iter):\n",
    "        output = model(perturbed_image)\n",
    "        loss = nn.CrossEntropyLoss()(output, target)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            perturbed_image += alpha * perturbed_image.grad.sign()\n",
    "            perturbed_image = torch.min(torch.max(perturbed_image, image - epsilon), image + epsilon)\n",
    "            perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "            perturbed_image.requires_grad_(True)\n",
    "    return perturbed_image\n",
    "\n",
    "# Step 4: Generate adversarial examples with PGD attack\n",
    "epsilon = 0.01  # Adjust this value as needed\n",
    "alpha = 0.01    # Adjust this value as needed\n",
    "num_iter = 20   # Adjust this value as needed\n",
    "\n",
    "successful_adversarial_examples = []\n",
    "corres_org_img=[]\n",
    "\n",
    "for input_image in input_images:\n",
    "    # Load the original image\n",
    "    original_image = Image.open(input_image).convert(\"RGB\")\n",
    "    \n",
    "    # Convert the PIL image to a PyTorch tensor and apply the necessary preprocessing\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    image_tensor = preprocess(original_image).unsqueeze(0)\n",
    "    image_tensor.requires_grad = True\n",
    "\n",
    "    # Forward pass to get the predictions\n",
    "    output = model(image_tensor)\n",
    "    original_label = torch.argmax(output, 1).item()\n",
    "\n",
    "    # Generate the target label for the PGD attack (choose any label other than the original label)\n",
    "    target_label = torch.randint_like(output, 0, output.shape[1])\n",
    "\n",
    "    # Generate the perturbed image using the PGD attack\n",
    "    perturbed_image = pgd_attack(model, image_tensor, target_label, epsilon, alpha, num_iter)\n",
    "\n",
    "    # Forward pass with the perturbed image\n",
    "    perturbed_output = model(perturbed_image)\n",
    "\n",
    "    # Check if the perturbed image's predicted label differs from the original label\n",
    "    perturbed_label = torch.argmax(perturbed_output, 1).item()\n",
    "    if perturbed_label != original_label:\n",
    "        # Save the perturbed image as a successful adversarial example\n",
    "        successful_adversarial_examples.append(perturbed_image)\n",
    "        corres_org_img.append(input_image)\n",
    "        # Save the original image to output_original_directory\n",
    "        os.makedirs(output_original_directory, exist_ok=True)\n",
    "        original_image_name = os.path.basename(input_image)\n",
    "        output_original_image_path = os.path.join(output_original_directory, original_image_name)\n",
    "        original_image.save(output_original_image_path)\n",
    "        \n",
    "        # Save the perturbed image to output_adversarial_directory\n",
    "        os.makedirs(output_adversarial_directory, exist_ok=True)\n",
    "        perturbed_image_name = f\"perturbed_{original_image_name}\"\n",
    "        output_perturbed_image_path = os.path.join(output_adversarial_directory, perturbed_image_name)\n",
    "        perturbed_image_pil = transforms.ToPILImage()(perturbed_image.squeeze(0))\n",
    "        perturbed_image_pil.save(output_perturbed_image_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccfc7f58-b51b-418c-959a-d00ada0a3852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of successful adversarial examples: 3566\n",
      "3566\n"
     ]
    }
   ],
   "source": [
    "# Print the length of successful adversarial examples\n",
    "print(\"Number of successful adversarial examples:\", len(successful_adversarial_examples))\n",
    "print(len(corres_org_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e12bd5c2-691a-4990-9922-9e9030e1ce05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Label ID: 148\n",
      "Original Label Name: ['n02071294', 'killer_whale']\n",
      "Original Confidence: 70.89036703109741\n",
      "Adversarial Label ID: 693\n",
      "Adversarial Label Name: ['n03873416', 'paddle']\n",
      "Adversarial Confidence: 70.52967548370361\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# Step 1: Load the pre-trained DenseNet121 model\n",
    "model = models.densenet121(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Load ImageNet class labels\n",
    "with open(\"C:/Users/DandaC8719/Documents/capstone_overall_code/imagenet_class_index.json\", \"r\") as f:\n",
    "    imagenet_classes = json.load(f)\n",
    "\n",
    "# Convert class labels to dictionary format\n",
    "imagenet_classes = {int(k): v for k, v in imagenet_classes.items()}\n",
    "\n",
    "# Step 3: Define the PGD attack function\n",
    "def pgd_attack(model, image, target, epsilon, alpha, num_iter):\n",
    "    perturbed_image = image.clone().detach().requires_grad_(True)\n",
    "    for i in range(num_iter):\n",
    "        output = model(perturbed_image)\n",
    "        loss = nn.CrossEntropyLoss()(output, target)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            perturbed_image += alpha * perturbed_image.grad.sign()\n",
    "            perturbed_image = torch.min(torch.max(perturbed_image, image - epsilon), image + epsilon)\n",
    "            perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "            perturbed_image.requires_grad_(True)\n",
    "    return perturbed_image\n",
    "\n",
    "# Step 4: Generate adversarial examples with PGD attack\n",
    "epsilon = 0.01  # Adjust this value as needed\n",
    "alpha = 0.01    # Adjust this value as needed\n",
    "num_iter = 20   # Adjust this value as needed\n",
    "\n",
    "def get_labels(image_path):\n",
    "    from PIL import Image  # Import Image module here\n",
    "    \n",
    "    # Load the original image\n",
    "    original_image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Convert the PIL image to a PyTorch tensor and apply the necessary preprocessing\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    image_tensor = preprocess(original_image).unsqueeze(0)\n",
    "    image_tensor.requires_grad = True\n",
    "\n",
    "    # Forward pass to get the predictions\n",
    "    output = model(image_tensor)\n",
    "    original_label_id = torch.argmax(output, 1).item()\n",
    "\n",
    "    # Calculate the confidence percentage for the original prediction\n",
    "    original_confidence = torch.softmax(output, dim=1)[0, original_label_id].item() * 100\n",
    "\n",
    "    # Get class names for original label\n",
    "    original_label_name = imagenet_classes[original_label_id]\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = nn.CrossEntropyLoss()(output, torch.tensor([original_label_id]))\n",
    "\n",
    "    # Zero all existing gradients\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backpropagate the loss to compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Collect the gradients of the input image\n",
    "    data_grad = image_tensor.grad.data\n",
    "\n",
    "    # Generate the target label\n",
    "    target = torch.tensor([original_label_id])\n",
    "\n",
    "    # Generate the perturbed image using the PGD attack\n",
    "    perturbed_image = pgd_attack(model, image_tensor, target, epsilon, alpha, num_iter)\n",
    "\n",
    "    # Forward pass with the perturbed image\n",
    "    perturbed_output = model(perturbed_image)\n",
    "\n",
    "    # Get the adversarial label\n",
    "    perturbed_label_id = torch.argmax(perturbed_output, 1).item()\n",
    "\n",
    "    # Calculate the confidence percentage for the adversarial prediction\n",
    "    perturbed_confidence = torch.softmax(perturbed_output, dim=1)[0, perturbed_label_id].item() * 100\n",
    "\n",
    "    # Get class names for adversarial label\n",
    "    perturbed_label_name = imagenet_classes[perturbed_label_id]\n",
    "    \n",
    "    return (original_label_id, original_label_name, original_confidence), (perturbed_label_id, perturbed_label_name, perturbed_confidence)\n",
    "\n",
    "\n",
    "image_path = \"./original_incorrect_predictions_folder(PGD)_DenseNet/ILSVRC2012_val_00024749.JPEG\"  \n",
    "(original_label_id, original_label_name, original_confidence), (adversarial_label_id, adversarial_label_name, adversarial_confidence) = get_labels(image_path)\n",
    "print(\"Original Label ID:\", original_label_id)\n",
    "print(\"Original Label Name:\", original_label_name)\n",
    "print(\"Original Confidence:\", original_confidence)\n",
    "print(\"Adversarial Label ID:\", adversarial_label_id)\n",
    "print(\"Adversarial Label Name:\", adversarial_label_name)\n",
    "print(\"Adversarial Confidence:\", adversarial_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e913c8-7687-4b16-930a-2b69f05f7ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
